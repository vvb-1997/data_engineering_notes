# Spark example

In the below code snippet, we are reading a data file and also inferring the schema. The result is a DataFrame. Then we are applying a chain of transformations. The result is another DataFrame. Finally, we are using the action.

```python
survery_df = spark.read \
	.option("header", "true") \
	.option("inferSchema", "true") \
	.csv(data_file)

partitioned_survey_df = survey_df.repartition(2)

count_df = partitioned_survey_df.where("Age < 40") \
	.select("Age", "Gender", "Country", "state") \
	.groupBy("Country") \
	.count()

count_df.collect()
```

Spark is similar to a compiler. It takes straightforward code and compiles it to generate low-level Spark code and also creates an execution plan. 

Replace this `show()` action with the `collect()` action and manually print the outcome to the application log. 
- The collect action returns the DataFrame as Python List. However, the `show()` method is a utility function to print the DataFrame. So, it is more likely that `collect()` action will be used, whereas the `show()` is useful for debugging purposes.
- The show() method compiles down to complex internal code. It might create unnecessary confusion when we are trying to learn a complex topic.

The `spark.sql.shuffle.partitions` is going to set the number of shuffle/sort partitions. The shuffle/sort caused by the `groupBy` will result in only two partitions. So, we made sure that the whole transformation flow is accomplished with two partitions.

```python
spark.sql.shuffle.partitions = 2
```

The overall pictorial representation of my Spark Application. Application code is compiled down into three jobs. These jobs are again broken into five stages. And then, these stages are further split into seven tasks. Spark Application will internally generate a bunch of Jobs, Stages, and tasks. And these tasks are the unit of work that is finally assigned to executors.

![[Spark J&S&T 1.png]]

# Execution Plan

## Read Data

The below spark code will only read CSV file without inferring schema and skipping first row as header. Loading data into a DataFrame is an action, and every Spark action is translated into a Spark job.

```python
survey_df = spark.read.csv(file_name)
```

Every job must have at least one stage, and each stage must have at least one task.

![[Spark Execution 1.png]]

![[Spark Execution 2.png]]

The DAG shows the sequence of internal processes. This sequence is the compiled code generated by the Spark.

![[Spark Execution 3.png]]

Below Spark code with options are to make sure that we actually read a portion of the file and infer the column names and data types for each column. Since we are reading the physical data, you can expect one more action. One more action should result in one more job.

```python
spark.read \
	.option("header", "true") \
	.option("inferSchema", "true") \
	.csv(data_file)
```

Both jobs are triggered by the CSV method. Because the high-level code is the CSV method, which internally causes two actions. The first action is to learn about the file and the partitions. The second action is to infer the file schema. Each job should have at least one stage and one task.

![[Spark Execution 4.png]]

DAG for stage-1. It looks like a complicated sequence. But I can easily make out that we are deserializing the content after scanning the text file. So, we are inferring the schema while we deserialize it.

![[Spark Execution 5.png]]
## Transformation

Spark is going to look at your code and break it into smaller sections separated by an action. We have two actions. Reading a file and collecting the results.  Each section becomes at least one Spark Job. Reading a file and inferring schema are two internal actions. Hence there are two jobs for this part of the code.

```python
survey_df = spark.read \
	.option("header", "true") \
	.option("inferSchema", "true") \
	.csv(data_file)

partitioned_survey_df = survey_df.repartition(2)
count_df = survey_df.filter("Age < 40") \
	.select("Age", "Gender", "Country", "state") \
	.groupBy("Country") \
	.count()
count_df.collect()
```

However, collecting the results is a simple action, we will get one job for this section of the code. And this job should cover everything from here to the `collect()`. So all these transformations, including `repartition`, `select`, `where`, `groupBy`, and `count`. All of this are planned as a single job. triggered by the collect action.

![[Spark Execution 6.png]]

Now let's drill down to the Spark Job. Spark will create a DAG for each job and break it into stages separated by a shuffle operation. So, here is the DAG for the `collect()` job. It start from reading the previous DataFrame, then `repartition` it, `filter`, `select`, `groupBy`, and finally `count` it.

![[Spark Execution 7.png]]

`Repartition` and `groupBy` causes shuffle. So Spark is going to break this job into three stages. This first stage will read the earlier DataFrame, make two partitions, and write them to an internal buffer called the exchange. The next stage is going to execute in parallel because now we have two partitions.

Each parallel stage task is going to read one partition from the exchange and perform these narrow transformations. That's what we call Tasks. Now the second stage finishes at the `groupBy` writing the result to exchange because we must shuffle/sort the `groupBy` results. The third stage will again start to read the result from the exchange. And this stage is also going to have two parallel tasks because we configured our session to have only two partitions for an internal shuffle.

Finally, the collect runs at the driver to collect outcomes from both these tasks. So, in summary, each action will result in a Job. Each wide-transformation will result in a separate stage and every stage executes in parallel depending upon the number of DataFrame partitions.

![[Spark Execution 8.png]]

The first stage operated on a single partition DataFrame, so we do not have any parallel processing here. The next two stages worked on two partitions, so we have two parallel tasks.

![[Spark Execution 9.png]]

In Spark UI we have three jobs. The first two jobs are for CSV action. The last job is collect action. The DAG for the whole job, there are three stages.
- The first stage starts reading a DataFrame. And finishes writing data to exchange caused by repartition. One task and a shuffle-write operation.
- The next stage starts reading the data from the exchange. Shuffle read and two tasks. So, this stage runs in parallel because we have two partitions for this stage. The stage finishes writing data to exchange caused by the `groupBy`.
- The last stage again reads from the exchange and executes two parallel tasks. 

![[Spark Execution 10.png]]
 
This DAG is for the whole job. If you want to drill down further into any of the stages, you can go to the stages tab or just click any of the stages here.